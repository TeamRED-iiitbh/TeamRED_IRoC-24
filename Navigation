# Navigation of Bot

In Ros2, we need to keep track of each frame of the robot and the environment relative to our other frames.
For example, we would need to know where a robot is relative to another robot or where a laser scan is relative to the map origin or to another part of the robots, etc..
The default solution to find this is to compute a 3d translation plus a 3d rotation between each existing frame.
Ros2 has TF2 package for this purpose.
The three most important TFs that has to be defined are map,scan(lidar) and base_link.
We get transforms from URDF (Unified Robot Description format).
 In the URDF, we will describe the different frames of the robot, for example, the base link and base scan frames and we will also describe the transform between those two frames then coming back to creating TF Well, actually we don't really need to create TFS ourselves.
Once we have written the URDF, we can start an existing node that we can find in almost all robots.
This node is the robot state publisher as input.It will receive the URDF and the joint State data published by the controller, 
for example,what is the position of the. Velocity of the wheels as an output.

The robot state publisher will then compute and publish the transforms for your robot, and this TF.


For example if we have urdf of a robot (my_robot.urdf).To visualize this we can use a ros2 package
sudo apt install ros-humble-urdf-tutorial
We should be able visualize by code:-
ros2 launch urdf_tutorial display.launch.py model:=/home/vivek/own_robot/my_robot.urdfmodel:=/home/vivek/own_robot/my_robot.urdfmodel:=/home/vivek/own_robot/my_robot.urdf

URDF is created to publish a base_link to base_scan TF for the navigation stack(map).



Now we need odom to base_link TF.
Odometry:-
 The odometry is used to localize a robot from its own starting point, using its own motion.(for example if you start walking from a place and count your steps you can know your location if distance covered by each step is estimated)
In odometry there will be drift over time or distance.(which will be compensated by map frame)
we want to compute where is the robot from its own starting point using the motion of the robot.
we can start with the most basic and minimal odometry that we can compute, which is by using the wheel encoders, A wheel encoder will tell us at what velocity the wheel is turning.By doing a simple integration calculation.We can also compute the position over the distance.
Then create a topic publisher and publish on the odom topic with the message type NAV message Odometry.(publish nav_msgs/odometry on /odom topic)

	Then publish odom â†’base link TF(on the /tf topic)

This all can be done using ros2_control framework of diff_drive_controller. But we need to configure that with our robot.


 If your robot have several sensors, which means several possible sources for computing a better and more precise odometry.Then an already existing package  robot localization package can be used.

We would then publish the odometry on the topic directly with ros2_control, and then we will publish the IMU data on the IMU topic.Then we run the localization node, which is going to subscribe to those topics and also merge and filter the data.The robot localization will publish the new ODOM on the automated filtered topic and it will also publish the order to base link transform for us.
For LIDAR we will need to read the data from the lidar and publish it on the scan topic with the message type sensor message laser scan.
Topic:- /scan
Message:- sensor_msgs/msg/LaserScan
Buy a ros2 compatible LIDAR and encoder
For other sensors you will have to do same things
First need to add a camera link in the URDF like we did with the base_scan for the lidar. Then we can read and publish the images on the camera topic using the sensor message image.

For a minimal setup wheel encoders and a LIDAR are enough sensor data.You can take the wheel encoder data through odometry and LIDAR data through /scan.
 Using ros2_control with the diff_drive_controller.This controller will take care of the closed loop control and will also publish the odometry and the required TF for ODOM.If you are using ros2_control, then we still need to write the code to communicate with your motors and encoders unless you find some ROS2 compatible encoders of course.Then all the logic of the closed loop control, the PID and everything else is handled for you.
If our robot is build following all this we can do navigation following these steps.
Lets assume we have made a robot similar to turtlebot3.
Install the slam package
sudo apt install ros-humble-slam-toolbox
To start navigation
ros2 launch nav2_bringup navigation_launch.py use_sim_time:=Trueros2 run nav2_map_server map_saver_cli -f maps/custom_robot
To start slam toolbox
ros2 launch slam_toolbox online_async_launch.py use_sim_time:=True
To start rviz
ros2 run rviz2 rviz2


You can save this configuration in rviz.
To make map use 
ros2 run turtlebot3_teleop teleop_keyboard
